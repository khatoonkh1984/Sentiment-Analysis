{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis: Determining the polarity of a text (positive or negative)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='imgs/QRNN.png' width='100%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[IMDB](http://ai.stanford.edu/~amaas/data/sentiment/) Dataset\n",
    "- A dataset for binary sentiment classification.\n",
    "- It provides a set of 25,000 highly polar movie reviews for training, and 25,000 for testing.\n",
    "\n",
    "\n",
    "**Note**: to run the following codes, you need to dowloand the dataset from the provided link and change the `data_dir` in the following cell accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import spacy  # just for NLP\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from utils import *\n",
    "from data_utils import Vocabulary, tokenizer\n",
    "from train_utils import train\n",
    "\n",
    "\n",
    "# setup\n",
    "NLP = spacy.load('en_core_web_sm')  # NLP toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Bromwell High is a cartoon comedy. \n",
    "It ran at the same time as some other programs about school life, such as 'Teachers'. \n",
    "My 35 years in the teaching profession lead me to believe that Bromwell High's \n",
    "satire is much closer to reality than is 'Teachers'. \n",
    "The scramble to survive financially, the insightful students who can see \n",
    "right through their pathetic teachers' pomp, the pettiness of the whole situation, \n",
    "all remind me of the schools I knew and their students. \n",
    "When I saw the episode in which a student repeatedly tried to burn down the school, \n",
    "I immediately recalled ......... at .......... High. \n",
    "A classic line: INSPECTOR: I'm here to sack one of your teachers. \n",
    "STUDENT: Welcome to Bromwell High. \n",
    "I expect that many adults of my age think that Bromwell High is far fetched. \n",
    "What a pity that it isn't!!!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bromwell High is a cartoon comedy.  It ran at the same time as some other programs about school life, such as 'Teachers'.  My 35 years in the teaching profession lead me to believe that Bromwell High's  satire is much closer to reality than is 'Teachers'.  The scramble to survive financially, the insightful students who can see  right through their pathetic teachers' pomp, the pettiness of the whole situation,  all remind me of the schools I knew and their students.  When I saw the episode in which a student repeatedly tried to burn down the school,  I immediately recalled ......... at .......... High.  A classic line  INSPECTOR  I'm here to sack one of your teachers.  STUDENT  Welcome to Bromwell High.  I expect that many adults of my age think that Bromwell High is far fetched.  What a pity that it isn't!!! \n"
     ]
    }
   ],
   "source": [
    "''' Remove the followimg characters and replace with space  '''\n",
    "text = re.sub(r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’;]\", \" \", str(text)) \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as 'Teachers'. My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is 'Teachers'. The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line INSPECTOR I'm here to sack one of your teachers. STUDENT Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!!! \n"
     ]
    }
   ],
   "source": [
    "'''Replace some spaces with one space'''\n",
    "text = re.sub(r\"[ ]+\", \" \", text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as 'Teachers'. My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is 'Teachers'. The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line INSPECTOR I'm here to sack one of your teachers. STUDENT Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't! \n"
     ]
    }
   ],
   "source": [
    "'''Replace some signs ! with one !'''\n",
    "\n",
    "text = re.sub(r\"\\!+\", \"!\", text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 'Bromwell', 'High', 'is', 'a', 'cartoon', 'comedy', '.', 'It', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', ',', 'such', 'as', \"'\", 'Teachers', \"'\", '.', 'My', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'Bromwell', 'High', \"'s\", 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', \"'\", 'Teachers', \"'\", '.', 'The', 'scramble', 'to', 'survive', 'financially', ',', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', 'teachers', \"'\", 'pomp', ',', 'the', 'pettiness', 'of', 'the', 'whole', 'situation', ',', 'all', 'remind', 'me', 'of', 'the', 'schools', 'I', 'knew', 'and', 'their', 'students', '.', 'When', 'I', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', ',', 'I', 'immediately', 'recalled', '.........', 'at', '..........', 'High', '.', 'A', 'classic', 'line', 'INSPECTOR', 'I', \"'m\", 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers', '.', 'STUDENT', 'Welcome', 'to', 'Bromwell', 'High', '.', 'I', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'Bromwell', 'High', 'is', 'far', 'fetched', '.', 'What', 'a', 'pity', 'that', 'it', 'is', \"n't\", '!']\n"
     ]
    }
   ],
   "source": [
    "'''tonenize'''\n",
    "\n",
    "tokens = [w.text for w in NLP.tokenizer(text)]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer and Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have defined a function in `utils.py`, which gets the inputs text and splits it to a sequence of tokens. We have used **SpaCy** toolkit for tokeniztion and you need to install it to run the codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```python\n",
    "def tokenizer(text):\n",
    "    text = re.sub(r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’;]\", \" \", str(text))\n",
    "    text = re.sub(r\"[ ]+\", \" \", text)\n",
    "    text = re.sub(r\"\\!+\", \"!\", text)\n",
    "    text = re.sub(r\"\\,+\", \",\", text)\n",
    "    text = re.sub(r\"\\?+\", \"?\", text)\n",
    "    return [x.text for x in NLP.tokenizer(text) if x.text != \" \"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_dir = 'dataset/aclImdb/'\n",
    "\n",
    "vocab_path = 'vocab.pkl'\n",
    "\n",
    "# parameters\n",
    "max_len = 200  # By this initilazatio we consider just 200 character of each text. we determine based on mean + 2 * sigma\n",
    "min_count = 10     #we replace every token which repeat less than 10 times with the spetial token. This is UNK = '<unk>'. \n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 25000 files [04:16, 97.52 files/s] \n"
     ]
    }
   ],
   "source": [
    "import splitfolders\n",
    "input_folder='dataset/aclImdb/test/'\n",
    "splitfolders.ratio(input_folder, output=\"dataset/aclImdb/valid\", seed=1337, ratio=(0.88, 0.1, 0.02)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'train']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir='dataset/aclImdb/dev/'\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(f'{data_dir}/train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f2e1e577ce4a12ba2fbe6d5eeb9421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min length = 9\n",
      "Max length = 1090\n",
      "Mean = 228.50\n",
      "Std  = 170.35\n",
      "mean + 2 * sigma = 569.19\n"
     ]
    }
   ],
   "source": [
    "all_filenames = glob(f'{data_dir}/*/*/*.txt')\n",
    "num_words = [len(open(f, encoding=\"utf-8\").read().split(' ')) for f in tqdm.notebook.tqdm(all_filenames)]\n",
    "\n",
    "# print statistics\n",
    "print('Min length =', min(num_words))\n",
    "print('Max length =', max(num_words))\n",
    "\n",
    "print('Mean = {:.2f}'.format(np.mean(num_words)))\n",
    "print('Std  = {:.2f}'.format(np.std(num_words)))\n",
    "\n",
    "print('mean + 2 * sigma = {:.2f}'.format(np.mean(num_words) + 2.0 * np.std(num_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>'  # special symbol we use for padding text\n",
    "UNK = '<unk>'  # special symbol we use for rare or unknown word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, path, tokenizer, \n",
    "                 split='train', \n",
    "                 vocab_path='vocab.pkl', \n",
    "                 max_len=100, min_count=10):\n",
    "        \n",
    "        self.path = path\n",
    "        assert split in ['train', 'test']\n",
    "        self.split = split\n",
    "        self.vocab_path = vocab_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.min_count = min_count\n",
    "        \n",
    "        self.cache = {}\n",
    "        self.vocab = None\n",
    "        \n",
    "        self.classes = []\n",
    "        self.class_to_index = {}\n",
    "        self.text_files = []\n",
    "        \n",
    "        split_path = f'{path}/{split}'\n",
    "        \n",
    "        for cls_idx, label in enumerate(os.listdir(split_path)):\n",
    "            text_files = [(fname, cls_idx) for fname in glob(f'{split_path}/{label}/*.txt')]\n",
    "            self.text_files += text_files\n",
    "            self.classes += [label]\n",
    "            self.class_to_index[label] = cls_idx\n",
    "        \n",
    "        self.num_classes = len(self.classes)\n",
    "            \n",
    "        # build vocabulary from training and validation texts\n",
    "        self.build_vocab()\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # read the tokenized text file and its label (neg=0, pos=1)\n",
    "        fname, class_idx = self.text_files[index]\n",
    "        \n",
    "        if fname in self.cache:\n",
    "            return self.cache[fname], class_idx\n",
    "        \n",
    "        # read text file \n",
    "        text = open(fname, encoding=\"utf-8\").read()\n",
    "        \n",
    "        # tokenize the text file\n",
    "        tokens = self.tokenizer(text.lower().strip())\n",
    "        \n",
    "        # padding and trimming\n",
    "        if len(tokens) < self.max_len:\n",
    "            num_pads = self.max_len - len(tokens)\n",
    "            tokens = [PAD] * num_pads + tokens\n",
    "        elif len(tokens) > self.max_len:\n",
    "            tokens = tokens[:self.max_len]\n",
    "            \n",
    "        # numericalizing\n",
    "        ids = torch.LongTensor(self.max_len)\n",
    "        for i, word in enumerate(tokens):\n",
    "            if word not in self.vocab.word2index:\n",
    "                ids[i] = self.vocab.word2index[UNK]  # unknown words\n",
    "            elif word != PAD and self.vocab.word2count[word] < self.min_count:\n",
    "                ids[i] = self.vocab.word2index[UNK]  # rare words\n",
    "            else:\n",
    "                ids[i] = self.vocab.word2index[word]\n",
    "                \n",
    "        # save in cache for future use\n",
    "        self.cache[fname] = ids\n",
    "        \n",
    "        return ids, class_idx\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text_files)\n",
    "    \n",
    "    def build_vocab(self):\n",
    "        if not os.path.exists(self.vocab_path):\n",
    "            vocab = Vocabulary(self.tokenizer)\n",
    "            filenames = glob(f'{self.path}/*/*/*.txt')\n",
    "            for filename in tqdm.notebook.tqdm(filenames, desc='Building Vocab'):\n",
    "                with open(filename, encoding='utf8') as f:\n",
    "                    for line in f:\n",
    "                        vocab.add_sentence(line.lower())\n",
    "\n",
    "            # sort words by their frequencies\n",
    "            words = [(0, PAD), (0, UNK)]\n",
    "            words += sorted([(c, w) for w, c in vocab.word2count.items()], reverse=True)\n",
    "\n",
    "            self.vocab = Vocabulary(self.tokenizer)\n",
    "            for i, (count, word) in enumerate(words):\n",
    "                self.vocab.word2index[word] = i\n",
    "                self.vocab.word2count[word] = count\n",
    "                self.vocab.index2word[i] = word\n",
    "                self.vocab.count += 1\n",
    "\n",
    "            pickle.dump(self.vocab, open(self.vocab_path, 'wb'))\n",
    "        else:\n",
    "            self.vocab = pickle.load(open(self.vocab_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_ds = TextClassificationDataset(data_dir, tokenizer, 'train', vocab_path, max_len, min_count)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_ds = TextClassificationDataset(data_dir, tokenizer, 'test', vocab_path, max_len, min_count)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0, 'pos': 1}"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.class_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg\n",
      "[   14     9    40   476     7   144     2  2236     7   213   116    30\n",
      "     2   176     4  3920     5   368     3    45    16    73   171   283\n",
      "   157   140     4     6   594   455     7     2   104  1174 14516  2030\n",
      "     7  1788  1640     5  1788  5044     3    41   147   259 13558   118\n",
      "   224   131    15    38    30  2196     7   124     3     5   124    82\n",
      "     4    49    28  1273    22    14    35     3   150    74   175   660\n",
      "   529     3     1    46   115   175   849  6226    21  1788  1640     3\n",
      "    46 21342  2813     2  2844     3  1999  2860    46  2399    21  1788\n",
      "  5044     5    74     2   154   804     4  1788  1640    16  2261  7362\n",
      "   421   633   170    14    25 18465    36     2  2702     3     5    13\n",
      "   155   137  1531    56     2  2143   960  5892    19   413    12    14\n",
      "    25    59     5   144     2  2143    83    31   219   295     2  2539\n",
      "   176   114    59    43  2062 21040     3   178    25    13   147   119\n",
      "    22   960  5892    55   101   400     2  2539   176  2101   114     3\n",
      "     5    36   228    12  2062    32   114  1083     3    71   103    11\n",
      "   666 16839    84   534    10    17    18    10   447     3    14     9\n",
      "   341   966   213  1164     4    45    30  3614]\n"
     ]
    }
   ],
   "source": [
    "ids, label = train_ds[0]\n",
    "\n",
    "print(train_ds.classes[label])\n",
    "print(ids.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is an example of why the majority of action films are the same . generic and boring , there 's really nothing worth watching here . a complete waste of the then barely tapped talents of ice t and ice cube , who 've each proven many times over that they are capable of acting , and acting well . do n't bother with this one , go see new jack city , <unk> or watch new york undercover for ice t , or boyz n the hood , higher learning or friday for ice cube and see the real deal . ice t 's horribly cliched dialogue alone makes this film grate at the teeth , and i 'm still wondering what the heck bill paxton was doing in this film ? and why the heck does he always play the exact same character ? from aliens onward , every film i 've seen with bill paxton has him playing the exact same irritating character , and at least in aliens his character died , which made it somewhat gratifying ... <br > < br > overall , this is second rate action trash . there are countless\n"
     ]
    }
   ],
   "source": [
    "# convert back the sequence of integers into original text\n",
    "print(' '.join([train_ds.vocab.index2word[i.item()] for i in ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example of why the majority of action films are the same. Generic and boring, there's really nothing worth watching here. A complete waste of the then barely-tapped talents of Ice-T and Ice Cube, who've each proven many times over that they are capable of acting, and acting well. Don't bother with this one, go see New Jack City, Ricochet or watch New York Undercover for Ice-T, or Boyz n the Hood, Higher Learning or Friday for Ice Cube and see the real deal. Ice-T's horribly cliched dialogue alone makes this film grate at the teeth, and I'm still wondering what the heck Bill Paxton was doing in this film? And why the heck does he always play the exact same character? From Aliens onward, every film I've seen with Bill Paxton has him playing the exact same irritating character, and at least in Aliens his character died, which made it somewhat gratifying...<br /><br />Overall, this is second-rate action trash. There are countless better films to see, and if you really want to see this one, watch Judgement Night, which is practically a carbon copy but has better acting and a better script. The only thing that made this at all worth watching was a decent hand on the camera - the cinematography was almost refreshing, which comes close to making up for the horrible film itself - but not quite. 4/10.\n"
     ]
    }
   ],
   "source": [
    "# print the original text\n",
    "print(open(train_ds.text_files[0][0]).read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vovcabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size = 29506\n",
      "\n",
      "Most common words:\n",
      "the: 666713\n",
      ",: 543467\n",
      ".: 470130\n",
      "and: 324156\n",
      "a: 321800\n",
      "of: 289313\n",
      "to: 267961\n",
      "is: 217022\n",
      ">: 202243\n",
      "it: 187974\n"
     ]
    }
   ],
   "source": [
    "vocab = train_ds.vocab\n",
    "freqs = [(count, word) for (word, count) in vocab.word2count.items() if count >= min_count]\n",
    "vocab_size = len(freqs) + 2  # for PAD and UNK tokens\n",
    "print(f'Vocab size = {vocab_size}')\n",
    "\n",
    "print('\\nMost common words:')\n",
    "for c, w in sorted(freqs, reverse=True)[:10]:\n",
    "    print(f'{w}: {c}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Classifier with Attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention computes a weighted average of the hidden states of the LSTM Model.\n",
    "# In fact, it produce a weight for each hidden state at different time steps\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, encoder_outputs):\n",
    "        # encoder_outputs = [batch size, sent len, hid dim]\n",
    "        energy = self.projection(encoder_outputs)\n",
    "        # energy = [batch size, sent len, 1]\n",
    "        weights = F.softmax(energy.squeeze(-1), dim=1)\n",
    "        # weights = [batch size, sent len]\n",
    "        outputs = (encoder_outputs * weights.unsqueeze(-1)).sum(dim=1)\n",
    "        # outputs = [batch size, hid dim]\n",
    "        return outputs, weights\n",
    "\n",
    "    \n",
    "class AttentionLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embed_size\n",
    "        self.num_layers = n_layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers,\n",
    "                            bidirectional=bidirectional, \n",
    "                            dropout= 0 if n_layers < 2 else dropout)\n",
    "        self.attention = SelfAttention(hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x = [sent len, batch size]\n",
    "        embedded = self.embedding(x)\n",
    "        # embedded = [sent len, batch size, emb dim]\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        # use 'batch_first' if you want batch size to be the 1st para\n",
    "        # output = [sent len, batch size, hid dim*num directions]\n",
    "        output = output[:, :, :self.hidden_dim] + output[:, :, self.hidden_dim:]\n",
    "        # output = [sent len, batch size, hid dim]\n",
    "        ouput = output.permute(1, 0, 2)\n",
    "        # ouput = [batch size, sent len, hid dim]\n",
    "        new_embed, weights = self.attention(ouput)\n",
    "        # new_embed = [batch size, hid dim]\n",
    "        # weights = [batch size, sent len]\n",
    "        new_embed = self.dropout(new_embed)\n",
    "        return self.fc(new_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29506\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 2 + len([w for (w, c) in train_ds.vocab.word2count.items() if c >= min_count])\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM parameters\n",
    "embed_size = 100\n",
    "hidden_size = 256 \n",
    "num_layers = 4\n",
    "\n",
    "# training parameters\n",
    "lr = 0.001\n",
    "num_epochs =10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttentionLSTM(vocab_size, embed_size, hidden_size, \n",
    "                      output_dim=train_ds.num_classes, \n",
    "                      n_layers=num_layers, bidirectional=True, dropout=0.3)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "criterion = criterion.to(device)\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.7, 0.99))\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  1/10] | Training Loss: 0.0139 | Testing Loss: 0.0138 | Training Acc:           50.16 | Testing Acc: 51.80\n",
      "[Epoch:  2/10] | Training Loss: 0.0132 | Testing Loss: 0.0116 | Training Acc:           61.16 | Testing Acc: 71.80\n",
      "[Epoch:  3/10] | Training Loss: 0.0104 | Testing Loss: 0.0113 | Training Acc:           75.12 | Testing Acc: 73.00\n",
      "[Epoch:  4/10] | Training Loss: 0.0083 | Testing Loss: 0.0113 | Training Acc:           81.64 | Testing Acc: 72.20\n",
      "[Epoch:  5/10] | Training Loss: 0.0064 | Testing Loss: 0.0111 | Training Acc:           86.84 | Testing Acc: 74.00\n",
      "[Epoch:  6/10] | Training Loss: 0.0050 | Testing Loss: 0.0117 | Training Acc:           89.64 | Testing Acc: 75.60\n",
      "[Epoch:  7/10] | Training Loss: 0.0037 | Testing Loss: 0.0166 | Training Acc:           93.08 | Testing Acc: 74.00\n",
      "[Epoch:  8/10] | Training Loss: 0.0027 | Testing Loss: 0.0172 | Training Acc:           95.52 | Testing Acc: 75.00\n",
      "[Epoch:  9/10] | Training Loss: 0.0022 | Testing Loss: 0.0183 | Training Acc:           95.92 | Testing Acc: 75.20\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f921efc8d4e455bb436fae0fda5ef2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "846f2f2d12e14cb38d8dad84ce0e2f19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist = train(model, train_dl, valid_dl, criterion, optimizer, device, scheduler, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "gist": {
   "data": {
    "description": "Untitled.ipynb",
    "public": false
   },
   "id": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
